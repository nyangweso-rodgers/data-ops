#version: "3"
services:
  ############################################################################
  # data-ops-mongodb-community-server
  #
  mongo-db-community-server:
    build:
      context: ./01-databases/02-mongodb-community-server
      dockerfile: Dockerfile
    container_name: mongo-db-community-server
    ports:
      - "27017:27017"
    #env_file: ./databases/02-mongodb-community-server/.env
    env_file:
      - .env
    environment:
      MONGODB_INITDB_ROOT_USERNAME: ${MONGODB_INITDB_ROOT_USERNAME}
      MONGODB_INITDB_ROOT_PASSWORD: ${MONGODB_INITDB_ROOT_PASSWORD}
    volumes:
      - mongo-db-community-server-volume:/data/db
    networks:
      - data-ops-network
  ############################################################################
  # postgres DB
  #
  postgres-db:
    build:
      context: ./01-databases/01-postgres
      dockerfile: Dockerfile
    container_name: postgres-db
    hostname: postgres
    restart: always
    ports:
      - "5432:5432"
    env_file:
      - .env
    environment:
      PGUSER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    #command:["postgres","-c","wal_level=logical","-c","max_replication_slots=10","-c","max_wal_senders=10",]
    #command: ["postgres", "-c", "wal_level=logical"]
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ./01-databases/01-postgres/init-scripts:/docker-entrypoint-initdb.d/ # Mount init scripts directory
    healthcheck:
      #test: ["CMD", "psql", "-U", "postgres", "-c", "SELECT 1"] 
      # or
      #test: ["CMD", "psql", "-U", "admin", "-c", "SELECT 1"]
      test: [
          "CMD-SHELL",
          "PGPASSWORD=${POSTGRES_PASSWORD} psql -U ${POSTGRES_USER} -d postgres -c 'SELECT 1'",
        ] # Use default database `postgres`
      interval: 10s
      timeout: 5s
      retries: 3
    #logging:
    #driver: "json-file"
    #options:
    #max-size: "2048m"
    networks:
      - data-ops-network
  ############################################################################
  # pgadmin
  #
  pgadmin:
    #image: dpage/pgadmin4:8.7 # https://hub.docker.com/layers/dpage/pgadmin4/8.7/images/sha256-198236db5d90e491354c351ccd02a21ce736030b72cd0f8b05430bc48a59d86b?context=explore
    image: dpage/pgadmin4:8.14.0 # https://hub.docker.com/r/dpage/pgadmin4/tags
    container_name: pgadmin
    restart: always
    ports:
      - "5050:80"
    env_file:
      - .env
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
    volumes:
      - pgadmin-volume:/var/lib/pgadmin
    depends_on:
      - postgres-db
    networks:
      - data-ops-network
  ############################################################################
  # mysql
  #
  mysql-db:
    build: ./01-databases/03-mysql # Build MySQL image from Dockerfile in the directory
    container_name: mysql-db # Name the container
    environment: # Set environment variables
      MYSQL_ROOT_PASSWORD: root # Set MySQL root password
      MYSQL_DATABASE: my_database # Create a database named my_database
      MYSQL_USER: user # Create a user named user
      MYSQL_PASSWORD: password # Set user password
    ports: # Expose MySQL port
      - "3306:3306" # Map container port 3306 to host port 3306
    volumes: # Attach volumes
      - ./mysql-db-volume:/var/lib/mysql # Attach volume to persist MySQL data in the same directory
  ############################################################################
  # zookeeper
  #
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    restart: always
    #restart: on-failure
    ports:
      - "2181:2181"
    environment:
      #- ALLOW_ANONYMOUS_LOGIN=yes
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-volume:/var/lib/zookeeper/data
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      #test: curl -s localhost:2181 || exit 1 # using curl to query the Zookeeper server status:
      start_period: 10s
      retries: 20
      interval: 10s
    networks:
      - data-ops-network
  ############################################################################
  # clickhouse-zookeeper
  #
  ############################################################################
  # kafka
  #
  kafka:
    build:
      context: ./03-messaging/01-apache-kafka
      dockerfile: Dockerfile
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    restart: always
    #restart: on-failure
    ports:
      # "`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-
      # An important note about accessing Kafka from clients on other machines:
      # -----------------------------------------------------------------------
      #
      # The config used here exposes port 9092 for _external_ connections to the broker
      # i.e. those from _outside_ the docker network. This could be from the host machine
      # running docker, or maybe further afield if you've got a more complicated setup.
      # If the latter is true, you will need to change the value 'localhost' in
      # KAFKA_ADVERTISED_LISTENERS to one that is resolvable to the docker host from those
      # remote clients
      #
      # For connections _internal_ to the docker network, such as from other services
      # and components, use broker:29092.
      #
      # See https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
      # "`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-
      #
      - "29092:29092"
      - "9092:9092"
      #- "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_DEFAULT_REPLICATION_FACTOR: "2"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # An important note about KAFKA_LISTENERS and KAFKA_ADVERTISED_LISTENERS
      # ---------------------------------------------------------------------
      # The KAFKA_LISTENERS setting defines where Kafka will listen for connections
      # The KAFKA_ADVERTISED_LISTENERS setting tells clients how to connect to the Kafka broker

      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092

      #KAFKA_LISTENERS: PLAINTEXT://kafka:29092, PLAINTEXT_HOST://0.0.0.0:9092
      #KAFKA_LISTENERS: PLAINTEXT://kafka:29092, PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT, PLAINTEXT_HOST:PLAINTEXT
      #KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT

      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100

      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2

      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2

      #KAFKA_JMX_PORT: 9092
      KAFKA_JMX_HOSTNAME: localhost

      #KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      #KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1

      #KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      #KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: "true"
      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"
    volumes:
      - kafka-volume:/var/lib/kafka/data
    healthcheck:
      test: nc -z localhost 9092 || exit -1
      start_period: 15s
      interval: 5s
      timeout: 10s
      retries: 10
    networks:
      - data-ops-network
  ############################################################################
  # kafka-ui
  #
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: always
    ports:
      - 8080:8080
    depends_on:
      - kafka
    environment:
      DYNAMIC_CONFIG_ENABLED: "true"
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMA_REGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper: 2181
    #volumes:
    #- ~/kui/config.yml:/etc/kafkaui/dynamic_config.yaml
    networks:
      - data-ops-network
  ############################################################################
  # schema-registry
  #
  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.1
    container_name: schema-registry
    hostname: schema-registry
    #restart: unless-stopped
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_started
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181

      #SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9101
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092

      #SCHEMA_REGISTRY_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9101

      #SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_DEBUG: "true"
    healthcheck:
      start_period: 10s
      interval: 10s
      retries: 20
      test: curl --user superUser:superUser --fail --silent --insecure http://localhost:8081/subjects --output /dev/null || exit 1
    networks:
      - data-ops-network
  ############################################################################
  # debezium
  #
  debezium-connect:
    image: debezium/connect:latest #https://hub.docker.com/r/debezium/connect
    restart: always
    container_name: debezium-connect
    hostname: debezium-connect
    depends_on:
      postgres-db:
        condition: service_healthy
      kafka:
        condition: service_healthy
    ports:
      - "8084:8084"
    environment:
      GROUP_ID: 1
      # Kafka config
      CONFIG_STORAGE_TOPIC: debezium-connect-config-storage
      STATUS_STORAGE_TOPIC: debezium-connect-status-storage
      OFFSET_STORAGE_TOPIC: debezium-connect-offset-storage
      BOOTSTRAP_SERVERS: kafka:29092
      CONFIG_STORAGE_REPLICATION_FACTOR: 2
      OFFSET_STORAGE_REPLICATION_FACTOR: 2
      STATUS_STORAGE_REPLICATION_FACTOR: 2
      #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      #CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      #KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema_registry:8081

      #CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      #CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      #VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema_registry:8081

      ENABLE_DEBEZIUM_SCRIPTING: "true"

      PLUGIN_PATH: /kafka/connect/plugins #added as part of the sink connector configuration
      GOOGLE_APPLICATION_CREDENTIALS: /etc/debezium/bigquery-keyfile.json # Specify the path inside the container
      #KAFKA_CONNECT_PLUGINS_DIR: /kafka/connect
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "--silent",
          "--fail",
          "-X",
          "GET",
          "http://localhost:8084/connectors",
        ]
      start_period: 10s
      interval: 10s
      timeout: 5s
      retries: 5
    #volumes:
    #- ./jar_files:/kafka/connect/avro_jar_files
    networks:
      - data-ops-network
  ############################################################################
  cp-kafka-connect: #https://hub.docker.com/r/confluentinc/cp-kafka-connect/tags
    #image: confluentinc/cp-kafka-connect:7.7.0
    build: ./03-message-broker/01-apache-kafka/01-connectors/01-cp-kafka-connect
    container_name: cp-kafka-connect
    hostname: cp-kafka-connect
    restart: always
    ports:
      - "8083:8083"
    depends_on:
      - kafka
      - schema-registry
      - zookeeper
      - postgres-db
    environment:
      # About CONNECT_BOOTSTRAP_SERVERS
      # -------------------------------
      # CONNECT_BOOTSTRAP_SERVERS informs cp-kafka-connect about the location of the Kafka broker it should connect to.
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_ADVERTISED_HOST_NAME: "cp-kafka-connect"
      CONNECT_GROUP_ID: "cp-kafka-connect-group"
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      ########## Storage Topics ##########
      # config.storage.topic
      #   The name of the Kafka topic where connector configurations are stored
      CONNECT_CONFIG_STORAGE_TOPIC: _cp-kafka-connect-config-storage
      CONNECT_OFFSET_STORAGE_TOPIC: _cp-kafka-connect-offset-storage
      CONNECT_STATUS_STORAGE_TOPIC: _cp-kafka-connect-status-storage

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1

      ########## Key Converters ##########
      #CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      #CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8085"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      #CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"

      ########## Value Converters ##########
      #CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      #CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"

      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"

      CONNECT_ZOOKEEPER_CONNECT: zookeeper:2181

      ########## SSL Configurations ##########
      ########## Additional Configurations ##########
      ########## Metric Reporter Configurations ##########

    ######### Volumes ##########
    volumes:
      - ./cp-kafka-connect/plugins/bigquery-keyfile.json:/etc/plugins/bigquery-keyfile.json
    #- ./cp-kafka-connect/connectors:/usr/share/confluent-hub-components
    #- ./cp-kafka-connect/config:/etc/kafka-connect
    #- ./cp-kafka-connect/postgresql-42.2.23.jar:/usr/share/java/postgresql-42.2.23.jar
    #command:
    networks:
      - data-ops-network
  ############################################################################
  # ksqldb-server
  #
  #ksqldb-server:
  #image: confluentinc/cp-ksqldb-server:latest
  #image: confluentinc/cp-ksqldb-server:7.5.3
  #hostname: ksqldb-server
  #container_name: ksqldb-server
  #depends_on:
  #- kafka
  #- schema-registry
  #ports:
  #- "8088:8088"
  #environment:
  #KSQL_CONFIG_DIR: "/etc/ksqldb"
  #KSQL_LISTENERS: http://0.0.0.0:8088
  #KSQL_BOOTSTRAP_SERVERS: kafka:29092
  #KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
  #KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
  #KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
  #KSQL_HOST_NAME: ksqldb-server
  #KSQL_CONNECT_URL: "http://connect:8083"
  #networks:
  #- data-ops-network
  ############################################################################
  # ksqldb-cli
  #
  #ksqldb-cli:
  #image: confluentinc/cp-ksqldb-cli:6.1.0
  #image: confluentinc/cp-ksqldb-cli:latest # usage -> docker-compose exec ksqldb-cli ksql http://ksqldb-server:8088
  #container_name: ksqldb-cli
  #depends_on:
  #- kafka
  #- debezium
  #- ksqldb-server
  #entrypoint: /bin/sh
  #tty: true
  #networks:
  #- data-ops-network
  ############################################################################
  # debezium-ui
  #
  debezium-ui:
    image: debezium/debezium-ui:latest
    restart: always
    container_name: debezium-ui
    hostname: debezium-ui
    depends_on:
      debezium-connect:
        condition: service_healthy
    ports:
      #- "8082:8080"
      - "8082:8082"
    environment:
      KAFKA_CONNECT_URIS: http://debezium:8084
    networks:
      - data-ops-network
  ############################################################################
  # apache-airflow
  #
  apache-airflow:
    build:
      context: ./05-pipeline/01-apache-airflow
      dockerfile: Dockerfile
    container_name: apache-airflow
    #command: webserver
    command: airflow standalone
    ports:
      #- "8080:8080"
      - "${APACHE_AIRFLOW_PORT}:8080"
    volumes:
      - apache-airflow-volume:/opt/airflow
      #- ./05-pipeline/01-apache-airflow/airflow_home:/opt/airflow_home # Mount the airflow_home directory, not the entire /opt/airflow
      - ./05-pipeline/01-apache-airflow/airflow_home/logs:/opt/airflow/logs
      - ./05-pipeline/01-apache-airflow/airflow_home/plugins:/opt/airflow/plugins
      - ./05-pipeline/01-apache-airflow/dags:/opt/airflow/dags
  ############################################################################
  # grafana
  #
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    hostname: grafana
    #restart: always
    restart: unless-stopped
    ports:
      - "3006:3006"
      #- "127.0.0.1:3000:3000"
    volumes:
      - grafana-volume:/var/lib/grafana
    user: "0:0"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      #- GF_INSTALL_PLUGINS=grafana-clickhouse-datasource
      - GF_DATABASE_TYPE=postgres
    depends_on:
      - postgres-db
    networks:
      - data-ops-network
  ############################################################################
  # prometheus
  #
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    #restart: always
    volumes:
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - data-ops-network
  ############################################################################
  # loki
  #
  loki:
    image: grafana/loki
    container_name: loki
    ports:
      - "3100:3100"
  ############################################################################
  # spring-boot-data-pipeline
  #
  #spring-boot-data-pipeline:
  #build:
  #context: ./spring-boot-data-pipeline
  #dockerfile: Dockerfile
  #image: spring-boot-data-pipeline
  #container_name: spring-boot-data-pipeline
  #ports:
  #- "9093:9093"
  #depends_on:
  #- kafka
  #environment:
  #SPRING_PROFILES_ACTIVE: dev
  #networks:
  #- data-ops-network
  ############################################################################
  # Node.js Kafka Client - Consumer & Producer
  #
  #node.js-kafka-client:
  #build:
  #context: ./apache-kafka/kafka-clients/node.js-kafka-client
  #dockerfile: Dockerfile
  #image: node.js-kafka-client
  #container_name: node.js-kafka-client
  #ports:
  #- "3004:3004"
  #depends_on:
  #- kafka
  #networks:
  #- data-ops-network
  ############################################################################
  # Python Kafka Client
  #
  #python-kafka-client:
  #build:
  #context: ./apache-kafka/kafka-clients/python-kafka-client
  #dockerfile: Dockerfile
  #image: python-kafka-client
  #container_name: python-kafka-client
  #ports:
  #- "3005:3005"
  #depends_on:
  #- kafka
  #networks:
  #- data-ops-network
  ############################################################################
  # clickhouse-node-1
  #
  clickhouse-node-1:
    #image: clickhouse/clickhouse-server:latest
    image: clickhouse/clickhouse-server:24.8.7.41
    container_name: clickhouse-node-1
    depends_on:
      - zookeeper
      - postgres-db
      #- pgadmin
    ports:
      - "9000:9000"
      #-'127.0.0.1:9000:9000'
      - "8123:8123"
      #- '127.0.0.1:8123:8123'
      #-- '127.0.0.1:9004:9004'
      - "9009:9009"
    restart: always
    ulimits:
      nproc: 65535
      nofile:
        soft: 262144
        hard: 262144
    links:
      - zookeeper
    volumes:
      - ./04-data-warehouse/01-clickhouse/config/config.xml:/etc/clickhouse-server/config.d/config.xml
      - ./04-data-warehouse/01-clickhouse/config/zookeeper.xml:/etc/clickhouse-server/config.d/zookeeper.xml
      - ./04-data-warehouse/01-clickhouse/config/access_management.xml:/etc/clickhouse-server/users.d/access_management.xml
      - ./04-data-warehouse/01-clickhouse/config/default-password.xml:/etc/clickhouse-server/users.d/default-password.xml
      - ./04-data-warehouse/01-clickhouse/config/storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./04-data-warehouse/01-clickhouse/config/cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      - ./04-data-warehouse/01-clickhouse/config/macros1.xml:/etc/clickhouse-server/config.d/macros.xml
      - clickhouse-node-1-volume:/var/lib/clickhouse/
      #- clickhouse-logs:/var/log/clickhouse-server/
    networks:
      - data-ops-network
  ############################################################################
  # metabase
  #
  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    depends_on:
      - postgres-db
      - pgadmin
    ports:
      - 3000:3000
    environment:
      #MB_DB_TYPE: ${MB_DB_TYPE}
      MB_DB_DBNAME: metabaseappdb
      MB_DB_PORT: 5432
      MB_DB_USER: metabase
      MB_DB_PASS: mypassword
      MB_DB_HOST: postgres
    healthcheck:
      test: curl --fail -I http://localhost:3000/api/health || exit 1
      interval: 15s
      timeout: 5s
      retries: 5
    networks:
      - data-ops-network
  ############################################################################
  # next.js survey app
  #
  #next.js-survey-app:
  #build:
  #context: ./web-apps/next.js-survey-app
  #dockerfile: Dockerfile
  #image: next.js-survey-app
  #container_name: next.js-survey-app
  #ports:
  #- "3002:3002"
  #depends_on:
  #- postgres-db
  #environment:
  #PGHOST: ${PGHOST}
  #POSTGRES_USER: ${POSTGRES_USER}
  #POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  #POSTGRES_SURVEY_DB: ${POSTGRES_SURVEY_DB}
  #env_file:
  #- ./web-apps/next.js-survey-app/.env
  #networks:
  #- data-ops-network
  ############################################################################
  # next.js app
  #
  #next.js-web-portal:
  #build:
  #context: ./next.js-web-portal
  #dockerfile: Dockerfile
  #image: next.js-web-portal
  #container_name: next.js-web-portal
  #ports:
  #- "3003:3003"
  #depends_on:
  #- mongodb-community-server
  #- postgres-db
  #restart: always
  #environment:
  #- MONGODB_URI_FOR_SURVEY_SERVICE=mongodb://${MONGO_INITDB_ROOT_USERNAME}:${MONGO_INITDB_ROOT_PASSWORD}@mongo:27017/survey-service?authSource=admin
  #MONGO_URI: ${MONGODB_COMMUNITY_SERVER_URI_FOR_SURVEY_SERVICE}
  #MONGODB_ATLAS_URI_FOR_SURVEY_SERVICE: ${MONGODB_ATLAS_URI_FOR_SURVEY_SERVICE}
  #PGHOST: ${PGHOST}
  #POSTGRES_USER: ${POSTGRES_USER}
  #POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  #POSTGRES_TEST_DB: ${POSTGRES_TEST_DB}
  #POSTGRES_TEST_DATABASE_URL: ${POSTGRES_TEST_DATABASE_URL}
  #env_file:
  #- ./next.js-web-portal/.env
  #volumes:
  #- /next.js-web-portal/src/app/prisma
  #command: sh -c "npx prisma migrate deploy && npm run dev"
  #entrypoint: ["sh", "/migrate.sh"]  # Execute migrate.sh script first
  #networks:
  #- data-ops-network
  ############################################################################
  # customers api with Express.js and postgres
  #
  customers-api:
    build:
      context: ./02-apis/01-customers-with-express.js-and-postgres-db
      dockerfile: Dockerfile
    image: customers-api
    container_name: customers-api
    ports:
      - "3001:3001"
    env_file:
      #- .env
      - ./02-apis/01-customers-with-express.js-and-postgres-db/.env
    depends_on:
      - postgres-db
      #- pgadmin
    networks:
      - data-ops-network
  ############################################################################
  # users api with Express.js and MongoDB
  #
  users-api:
    build:
      context: ./02-apis/02-users-with-express.js-and-mongo-db
      dockerfile: Dockerfile
    image: users-api
    container_name: users-api
    ports:
      - "3002:3002"
    env_file:
      #- ./apis/02-users-api-with-express.js-and-mongo-db/.env
      - .env # Reference the root .env file here
    depends_on:
      - mongo-db-community-server
    networks:
      - data-ops-network
############################################################################
#Superset
#
#superset:
#image: alexmerced/dremio-superset
#container_name: superset
#ports:
#- "8088:8088"
#environment:
#- ADMIN_USERNAME=admin
#- ADMIN_EMAIL=admin@superset.com
#- ADMIN_PASSWORD=admin
############################################################################
# networks
#
networks:
  data-ops-network:
    name: data-ops-network
    driver: bridge
############################################################################
# volumes
#
volumes:
  mongo-db-community-server-volume:
    name: mongodb-community-server-volume
    #driver: local
  data-ops-mongodb-volume:
    name: data-ops-mongodb-volume
    #driver: local
  postgres-db-volume:
    name: postgres-db-volume
    #driver: local
    #driver_opts:
    #type: none
    #o: bind
    #device: ./data
  clickhouse-node-1-volume:
    name: clickhouse-node-1-volume
    #driver: local
  pgadmin-volume:
    name: pgadmin-volume
    #driver: local
  zookeeper-volume:
    name: zookeeper-volume
  kafka-volume:
    name: kafka-volume
  grafana-volume:
    name: grafana-volume
  prometheus-data:
    name: prometheus-data
  apache-airflow-volume:
    name: apache-airflow-volume
    #driver: local  # Uncomment if necessary, depending on your setup
